<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>pika</title>


<style type="text/css">
*{margin:0;padding:0;}
body {
	font:13.34px helvetica,arial,freesans,clean,sans-serif;
	color:black;
	line-height:1.4em;
	background-color: #F8F8F8;
	padding: 0.7em;
}
p {
	margin:1em 0;
	line-height:1.5em;
}
table {
	font-size:inherit;
	font:100%;
	margin:1em;
}
table th{border-bottom:1px solid #bbb;padding:.2em 1em;}
table td{border-bottom:1px solid #ddd;padding:.2em 1em;}
input[type=text],input[type=password],input[type=image],textarea{font:99% helvetica,arial,freesans,sans-serif;}
select,option{padding:0 .25em;}
optgroup{margin-top:.5em;}
pre,code{font:12px Monaco,"Courier New","DejaVu Sans Mono","Bitstream Vera Sans Mono",monospace;}
pre {
	margin:1em 0;
	font-size:12px;
	background-color:#eee;
	border:1px solid #ddd;
	padding:5px;
	line-height:1.5em;
	color:#444;
	overflow:auto;
	-webkit-box-shadow:rgba(0,0,0,0.07) 0 1px 2px inset;
	-webkit-border-radius:3px;
	-moz-border-radius:3px;border-radius:3px;
}
pre code {
	padding:0;
	font-size:12px;
	background-color:#eee;
	border:none;
}
code {
	font-size:12px;
	background-color:#f8f8ff;
	color:#444;
	padding:0 .2em;
	border:1px solid #dedede;
}
img{border:0;max-width:100%;}
abbr{border-bottom:none;}
a{color:#4183c4;text-decoration:none;}
a:hover{text-decoration:underline;}
a code,a:link code,a:visited code{color:#4183c4;}
h2,h3{margin:1em 0;}
h1,h2,h3,h4,h5,h6{border:0;}
h1{font-size:170%;border-top:4px solid #aaa;padding-top:.5em;margin-top:1.5em;}
h1:first-child{margin-top:0;padding-top:.25em;border-top:none;}
h2{font-size:150%;margin-top:1.5em;border-top:4px solid #e0e0e0;padding-top:.5em;}
h3{margin-top:1em;}
hr{border:1px solid #ddd;}
ul{margin:1em 0 1em 2em;}
ol{margin:1em 0 1em 2em;}
ul li,ol li{margin-top:.5em;margin-bottom:.5em;}
ul ul,ul ol,ol ol,ol ul{margin-top:0;margin-bottom:0;}
blockquote{margin:1em 0;border-left:5px solid #ddd;padding-left:.6em;color:#555;}
dt{font-weight:bold;margin-left:1em;}
dd{margin-left:2em;margin-bottom:1em;}
sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}
kbd {
  display: inline-block;padding: 3px 5px;font-size: 11px;line-height: 10px;color: #555;vertical-align: middle;background-color: #fcfcfc;border: solid 1px #ccc;border-bottom-color: #bbb;border-radius: 3px;box-shadow: inset 0 -1px 0 #bbb;
}
* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
}
</style>


</head>

<body>

<h2 id="toc_0">Pika 笔记</h2>

<hr>

<p><em>written by Alex Stocks on 2018/09/07，版权所有，无授权不得转载</em></p>

<h3 id="toc_1">0 引言</h3>

<hr>

<p>愚人所在公司的大部分服务端业务无论是缓存还是存储颇为依赖 Codis，经过数次踩坑，其中一条经验教训是：线上 Redis 数据不要落地。</p>

<p>也就是说，我司的 Codis 集群中的 Redis，无论是 master 还是 slave，都没有打开 rdb 和 aof，所有数据都放在内存中。Codis 以这种方式“平静地”运行了一年，但是大伙终究心里石头无法落地，现状要求运维的同事在线上部署一种能高效运行且数据能落地的 “Codis”。</p>

<p>经交流和调研，今年七月份运维的同事决定采用 v2.3.x Pika 版的 Codis【下文提及的 Pika 不做特殊说明均指代 Pika 版本的 Codis 集群，pika 则指代单个 pika member】。在经过一段时间测试后，结果也令人满意：无论是在 SATA 盘还是 SSD 盘上，写【set，key 长度 16B， value 长度 30B】 qps 最差 60k/s，稳定情况下 80k/s，峰值可达 100k/s。于是 CTO 便拍板决定继续测试【到目前为止运维同事已经各种测试了两个月】，并根据公司以往的传统：使用开源系统，公司内部必须有人通读其代码，且能够解决掉在测试和线上遇到的问题。</p>

<p>最终这个“光荣任务”落在了愚人肩上。本文用来记录我阅读代码并在改进 Pika 【到 2018/09/07 为止主要是开发相关工具】过程中遇到的一些问题。</p>

<h3 id="toc_2">1 数据迁移</h3>

<hr>

<p>八月初运维的同事提出了一个需求：把 Pika 数据实时同步到 Codis 集群，即把 Pika 集群作为数据固化层，把 Codis 作为数据缓存层。</p>

<h4 id="toc_3">1.1 Pika-port V1</h4>

<p>刚开始得到这个需求，愚人的实现思路是：</p>

<ul>
<li>1 通过 redis-cli 向 pika 发送 bgsave 命令，然后把 dump 出来的数据解析后发送给 Codis；</li>
<li>2 再开发这样一个工具：根据 dump info 文件存储的 filenum &amp; offset 信息解析 binlog，并把解析出来的写指令增量同步给 Codis。</li>
</ul>

<p>根据这个思路，借鉴<a href="https://github.com/Qihoo360/pika/wiki/%E4%BD%BF%E7%94%A8binlog%E8%BF%81%E7%A7%BB%E6%95%B0%E6%8D%AE%E5%B7%A5%E5%85%B7">参考文档1</a>开始实现V1 版本的工具【模仿 redis-port，愚人命名为 pika-port】。但在开发到最后一步时遇到这个问题：pika 以 mmap 方式向磁盘写入 binlog，redis-port 只需要读 binlog，而一般存储系统的读速度最低 5 倍于写速递，当 redis-port 追上 pika 的最新 binlog 文件数据后， 很可能读到截断的脏数据！</p>

<p>因当时刚开始读 pika 代码，遇到这个无法解决坎后便只能放弃这个方案了【后来把pika/src/pika<em>binlog</em>sender_thread.cc 详细读懂后已经找到了解决方法，但此时 pika-port V2版本已经开发完毕】。</p>

<p>V1 虽然半途而废，但是开发过程中遇到的两个问题比较有意思，V2 版开发时也需要处理，所以记录如下：</p>

<ul>
<li><p>pika 的 binlog record 在每个 redis 写命令后面追加了四个额外信息，分别是：Pika Magic [kPikaBinlogMagic]、server_id【用于双 master 同步时做去重】、binlog info【主要是执行命令的时间】以及 send hub 信息，需要过滤掉；</p>

<blockquote>
<p>代码详见 include/pika_command.h:Cmd::AppendAffiliatedInfo，修改后的 redis 命令 <code>set A 1</code> 格式为 <code>*7\r\n$3\r\nset\r\n$1\r\nA\r\n$1\r\n1\r\n$14\r\n__PIKA_X#$SKGI\r\n$1\r\n1\r\n$16\r\nj[m\r\n$1\r\n1\r\n</code></p>

<p>这些补充信息在跨机房数据同步的情况下也很有用，详细内容见<a href="http://kernelmaker.github.io/pika-muli-idc">参考文档7</a></p>
</blockquote></li>
<li><p>pika 内部有一个特殊的 set 用于记录当前 migrate 信息，set key 前缀是 <code>_internal:slotkey:4migrate:</code>，这个在进行数据同步时也需要过滤掉；</p></li>
</ul>

<h4 id="toc_4">1.2 Pika-port V2</h4>

<p>V2 版本的 pika-port 相当于是 pika 和 Codis / Redis 之间的 proxy，实现流程是：</p>

<ul>
<li>1 pika-port 启动时候伪装从 pika 的 slave，向 pika 发送 trysync 指令，如<code>trysync 10.33.80.155 20847 0 0</code>，10.33.80.155:20847 为 pika-port 的启动监听地址，后两个参数分别为 filenum 和 offset，同时监听 +1000 地址;</li>
<li>2 pika-port 收到 pika 发来的 wait ack 后，监听 +3000 端口，启动 rsync deamon 等待全量数据同步；</li>
<li>3 pika-port 循环检测 dump info 文件是否存在，当检测到 info 文件存在时，意味着全量同步数据完成，此时阻塞所有流程并把收到的全量数据发送给 Codis；</li>
<li>4 pika-port 根据 info 文件提供的 filenum 和 offset 再次向 pika 发送 trysync 指令，并根据 pika 回复的 ack 获取到 sid 作为此次连接的标识；</li>
<li>5 pika-port 监听 +2000 端口，启动心跳发送线程，首先向 pika 发送 <code>spci sid</code>指令，然后每个 1s 向 pika 发送 <code>ping</code>指令，并等待 pika 回复的 <code>pong</code> ack；</li>
<li>6 pika-port 启动一个 Codis 连接线程池【本质是一个线程池，每个线程启动一个 Codis 连接】；</li>
<li>6 pika 收到心跳后，向 pika-port 发送 <code>auth sid</code> 指令成功后，就循环解析 binlog 并把数据增量同步给 pika-port；</li>
<li>7 pika-port 收到 pika 实时同步过来的单个 redis 写指令，过滤掉其中非法指令，再删除合法质量中结尾四个辅助信息，然后根据写指令中的 key 进行 hash 计算后交给线程池中某个线程，此线程将会以阻塞方式将此数据同步给 Codis 直至成功。</li>
</ul>

<p>整个流程需要对 pika 的主从复制流程非常熟悉，关于主从复制流程可以详细阅读<a href="https://www.jianshu.com/p/01bd76eb7a93">参考文档2</a>。目前 pika-port 已经开发完毕，支持 v2.3.6 版本的 pika数据实时迁移到 Codis/Redis。</p>

<p>在开发过程中遇到了一些坑，有的是自己对 pika 理解不透彻，有的是 pika 自身一些缺陷，下面详细分小节记录之，以备将来作参考之用。</p>

<h5 id="toc_5">1.2.1 rsync 启动失败</h5>

<p>Pika-port 与 pika 之间全量数据同步是通过 rsync 进行的，如果 pika-port 启动 rsync 失败【譬如rsync 监听端口被占用】，pika-port 所借鉴的 <a href="https://github.com/qihoo360/pika/blob/master/src/pika_trysync_thread.cc#L259">PikaTrysyncThread::ThreadMain</a> 仅仅记录一个错误日志，然后继续相关流程。</p>

<p>合理的处理方法当然是启动 rsync daemon 失败退出即可，然官方相关处理流程如是，且出现这种错误概率极低，愚人处理方法就是暂时不处理这种 corner case。</p>

<h5 id="toc_6">1.2.2 非法命令过滤</h5>

<p>Pika-port 会对 pika 发来的 redis 写指令进行非法性检查，过滤掉 command 为 auth 以及 key 为 <code>_internal:slotkey:4migrate:</code>前缀的非法指令。</p>

<p>在开发过程中，对非法指令的过滤是 <a href="https://github.com/divebomb/pika/blob/master/tools/pika_port/master_conn.cc">MasterConn::DealMessage</a> 处理的，过滤功能开发到是很简单，但是在开发测试过程中遇到这样一个坑：一旦 pika-port 遇到一个非法指令过滤掉后，pika 与 pika-port 之间的连接就断开发并疯狂重新建立连接。</p>

<p>经过对 <a href="https://github.com/PikaLabs/pink/blob/master/pink/src/redis_conn.cc">RedisConn::ProcessInputBuffer</a> 详细分析后才发现问题所在： <a href="https://github.com/divebomb/pika/blob/master/tools/pika_port/master_conn.cc">MasterConn::DealMessage</a> 遇到非法字符串后返回了一个负值作为错误标识，而 <a href="https://github.com/PikaLabs/pink/blob/master/pink/src/redis_conn.cc">RedisConn::ProcessInputBuffer</a> 调用这个函数后如果检测到结果是负值，就认为处理出错，最终会导致连接被关闭。</p>

<p>最终的解决方法当然是把返回结果改为 0 就可以了。</p>

<h5 id="toc_7">1.2.3 主从复制过程丢失数据</h5>

<p>Pika-port V2开发完毕后测试过程中，遇到这样一个 corner case：通过 redis-cli 向 pika 写入 A 指令【譬如 set A 1】，在 60s 之后再次向 pika 写入 B 指令【譬如 set B 2】，然后立即写入 C 指令【譬如 set C 3】，最后 Codis/Redis 中只有 A 和 C 指令的数据，把 B 质量的数据丢了！</p>

<p>通过 tcpdump 在 pika 和 pika-port 之间进行抓包，分别得到如下两个关键结果【由于花费了半天时间不断重复测试以分析网络流程，所以两幅图时间先后有些错乱，不必较真】：</p>

<p><img src="../pic/pika_tcp_fin_reset.jpg" alt=""></p>

<div><pre><code class="language-none">                                                   ***图1: pika-port fin reset***</code></pre></div>

<p><img src="../pic/pika_tcp_reset_3handshake.jpg" alt=""></p>

<div><pre><code class="language-none">                                                   ***图2: pika与pika-port 3 handshake***</code></pre></div>

<p>图1 是在 pika 向 pika-port 写入 B 指令时的网络流程，通过分析 图1 并结合相关代码分析，可以得到这样一个流程：</p>

<ul>
<li>在 60s 的时间间隔内 pika 未向 pika-port 同步数据，导致 pika-port 的超时检查函数 <a href="https://github.com/PikaLabs/pink/blob/master/pink/src/holy_thread.cc#L189">HolyThread::DoCronTask</a>认为连接超时，便向 pika 发送 fin1 包后，把连接关闭了；</li>
<li>pika 向 pika-port 写入 B 指令时，pika-port 向 pika 回复了 reset 信号。</li>
</ul>

<p>图2 则是 pika 向 pika-port 写入 C 指令的网络流程，同样分析后得到其流程是：</p>

<ul>
<li>pika 收到 pika-port 发来的 reset 信号并未处理，继续向 pika-port 发送 C 指令；</li>
<li>pika PikaBinlogSenderThread 此时方能判断出连接已经被 pika-port 关闭(https://github.com/qihoo360/pika/blob/master/src/pika<em>binlog</em>sender_thread.cc#L309)，然后关闭连接并重新建立与 pika-port 之间的数据同步连接，并重复发送 C 指令，此次成功。</li>
</ul>

<p>从 <a href="https://github.com/qihoo360/pika/blob/master/src/pika_binlog_sender_thread.cc#L241">PikaTrysyncThread::ThreadMain</a> 整个流程可以得出这样一个结论：pika 调用 write api 向 pika-port 写 B 指令的时候，并没有进行读操作以判断当前是否收到了 pika-port 发来的 rst 包，只是调用 write api 向 pika-port 进行了写，并根据其返回值为0就认为写成功了，进而理所当然的认为对端也能收到 B 指令。</p>

<p>可能有些对 tcp 四次挥手逻辑不甚明了的人对这个过程有些不甚了了，根本原因是 tcp 是双向连接，pika-port 只是关闭了 pika-port --&gt; pika 这个方向的连接，而 pika --&gt; pika-port 这个方向的单向连接还是存在的，只不过 pika-port 依赖的 pink 网络库在关闭一个单向连接时调用了 close 函数，导致结果是：pika-port 关闭了 pika-port --&gt; pika 这个方向的连接的同时不再接收 pika --&gt; pika-port 这个方向由 pika 发来的 B 指令数据！</p>

<p>解决问题的根本就在于正确处理 RST 信号，linux manpage 对 RST 信号的处理解释如下：</p>

<div><pre><code class="language-none">What happens if the client ignores the error return from readline and writes more data to the server? This can happen, for example, if the client needs to perform two writes to the server before reading anything back, with the first write eliciting the RST.

The rule that applies is: When a process writes to a socket that has received an RST, the SIGPIPE signal is sent to the process. The default action of this signal is to terminate the process, so the process must catch the signal to avoid being involuntarily terminated.

If the process either catches the signal and returns from the signal handler, or ignores the signal, the write operation returns EPIPE.</code></pre></div>

<p>上面很清晰的说明：写 B 指令时如果不读取 RST 相关错误信令，写 C 指令时 write 会返回 broken pipe 错误。所以正确的处理方法应该是：在进行 write 之前进行一次 read，以判断对端是否已经发来 fin 包；或者在 write 之后进行 read 以判断对端是否发来 rst 包。</p>

<p>考虑到 <a href="https://github.com/qihoo360/pika/blob/master/src/pika_binlog_sender_thread.cc#L241">PikaTrysyncThread::ThreadMain</a> 向 pika-port 发送数据的方式是 one way 的，pika-port 自身不会给 pika 回复任何消息，所以第二种方法成本略高。再考虑到这种情况是因为两个写指令之间写时间间隔太长所致，更进一步地处理方法是：每次调用 write 之后记录本次 write 执行的时间，下一次调用 write 时把系统当前时间与上一次 write 的时间进行比较，如果时间间隔超过某个阈值【譬如 1s】，则需要先进行读操作，判断出 pika-port --&gt; pika 方向的连接正常，再调用 write 进行 pika --&gt; pika-port 方向的数据写操作。</p>

<p>根据这个方案的相关改进代码写完，并已向 pika 官方提交了 <a href="https://github.com/PikaLabs/pink/pull/30">pr</a>，有待 merge。</p>

<p>在测试过程中，发现 pika 自身的 master 和 slave 进行数据复制时，并不会出现数据丢失的错误。经过加 log 分析，愚人在今日[2018/09/08] 下午 15:50pm 发现原因所在：pika slave 并不会对 pika master 之间的数据复制连接进行超时判断，仅仅依靠 tcp 自身的 KeepAlive 特性对连接进行保活【个人认为这种处理方法是不理智的】。至于代码层次原因，详见下图：</p>

<p><img src="../pic/pika_keepalive_check.png" alt=""></p>

<p>Pika-port 调用了上图<a href="https://github.com/pikalabs/pink/blob/master/pink/src/holy_thread.cc#L16">第一个构造函数</a>，直接导致  HolyThread::keepalive<em>time</em> 参数被赋值 60，进而导致<a href="https://github.com/pikalabs/pink/blob/master/pink/src/holy_thread.cc#L186">HolyThread::DoCronTask</a> 超时检查逻辑被激活，然后 pika-port 与 pika 之间连接被 pika-port 关闭。</p>

<p>而 pika 自身则是调用上图的<a href="https://github.com/pikalabs/pink/blob/master/pink/src/holy_thread.cc#L25">第二个构造函数</a>，直接导致  HolyThread::keepalive<em>time</em> 参数在被 gcc 编译时候被赋值 0，然后 pika slave 就不会去对它与 pika master之间连接作任何超时检查，所以也就不会出现丢数据的问题！</p>

<p>恰当的处理方法当然是重构两个构造函数，让其行为一致，然而作为著名项目的已有代码，相关改动牵一发而动全身，最终处理方法是我在 <a href="https://github.com/PikaLabs/pink/pull/31">pr</a>【对网络fd进行读写须用 recv，如果用 pread 则会收到 ESPIPE 错误】 中对相关函数所在的头文件中加上注释以进行<a href="https://github.com/divebomb/pink/blob/master/pink/include/server_thread.h#L195">调用提醒</a>。</p>

<p>至于为何要依赖 tcp 自身的 keepalive 机制而不是在逻辑层对 tcp 连接进行超时判断，pika 开发者陈宗志给出了一个 <a href="http://baotiao.github.io/tech/2015/09/25/tcp-keepalive/">blog</a> 进行解释，仁者见仁智者见智，这个就不再次探讨了。</p>

<p>在处理这个问题时，与胡伟、<a href="https://github.com/zhengshuxin">郑树新</a>、<a href="https://github.com/loveyacper">bert</a>、<a href="https://github.com/git-hulk">hulk</a>等一帮老友进行了相关探讨，受益匪浅，在此一并致谢！</p>

<h3 id="toc_8">2 数据备份</h3>

<hr>

<p>Pika 官方 wiki [<a href="https://github.com/qihoo360/pika/wiki/pika-%E5%BF%AB%E7%85%A7%E5%BC%8F%E5%A4%87%E4%BB%BD%E6%96%B9%E6%A1%88">参考文档4</a>] 有对其数据备份过程的图文描述，此文就就不再进行转述。</p>

<p>Ardb 作者对 Pika 的评价是  “直接修改了rocksdb代码实现某些功能。这种做法也是双刃剑，改动太多的话，社区的一些修改是很难merge进来的”【详见<a href="http://yinqiwen.github.io/">参考文档5</a>】。与比较几个主流的基于 RocksDB 实现的 KV 存储引擎（如 TiKV/SSDB/ARDB/CockroachDB）作比较，Pika 确实对 RocksDB 的代码侵入比较严重。RocksDB 默认的备份引擎 BackupEngine 通过 <code>BackupEngine::Open</code> 和 <code>BackupEngine::CreateNewBackup</code> 即实现了数据的备份【关于RocksDB 的 Backup 接口详见 <a href="http://alexstocks.github.io/html/rocksdb.html">参考文档6</a> 6.8节】，而 Pika 为了效率起见重新实现了一个 <code>nemo::BackupEngine</code>，以进行异步备份。</p>

<p>Pika 的存储引擎 nemo 依赖于其对 RocksDB 的封装引擎 nemo-rocksdb，下面结合<a href="https://github.com/qihoo360/pika/wiki/pika-%E5%BF%AB%E7%85%A7%E5%BC%8F%E5%A4%87%E4%BB%BD%E6%96%B9%E6%A1%88">参考文档4</a> 从代码层面对备份流程进行详细分析。</p>

<p><font size=“2” color=blue><strong><em>注：本章描述的备份流程基于 pika 的 nemo 引擎，基本与最新的 blackwidow 引擎的备份流程无差。</em></strong></font></p>

<h4 id="toc_9">2.1 DBNemoCheckpoint</h4>

<hr>

<p>nemo:DBNemoCheckpoint 提供了执行实际备份任务的 checkpoint 接口，其实际实现是 nemo:DBNemoCheckpointImpl，其主要接口如下：</p>

<div><pre><code class="language-cpp">class DBNemoCheckpointImpl : public DBNemoCheckpoint {
  // 如果备份目录和源数据目录在同一个磁盘上，则对 SST 文件进行硬链接，
  // 对 manifest 文件和 wal 文件进行直接拷贝
  virtual Status CreateCheckpoint(const std::string&amp; checkpoint_dir) override;
  // 先阻止文件删除【rocksdb:DB::DisableFileDeletions】，然后获取 rocksdb:DB 快照，如 db 所有文件名称、
  // manifest 文件大小、SequenceNumber 以及同步点(filenum &amp; offset)
  //
  // nemo:BackupEngine 把这些信息组织为BackupContent
  virtual Status GetCheckpointFiles(std::vector&lt;std::string&gt; &amp;live_files,
      VectorLogPtr &amp;live_wal_files, uint64_t &amp;manifest_file_size,
      uint64_t &amp;sequence_number) override;

  // 根据上面获取到的 快照内容 进行文件复制操作
  virtual Status CreateCheckpointWithFiles(const std::string&amp; checkpoint_dir,
      std::vector&lt;std::string&gt; &amp;live_files, VectorLogPtr &amp;live_wal_files,
      uint64_t manifest_file_size, uint64_t sequence_number) override;
}</code></pre></div>

<p><code>CreateCheckpoint</code> 接口可以认为是同步操作，它通过调用 <code>GetCheckpointFiles</code> 和 <code>CreateCheckpointWithFiles</code> 实现数据备份。</p>

<p><code>DBNemoCheckpointImpl::GetCheckpointFiles</code> 先执行 “组织文件删除”，然后再获取 快照内容。</p>

<p><code>DBNemoCheckpointImpl::CreateCheckpointWithFiles(checkpoint_dir, BackupContent)</code> 详细流程:</p>

<ul>
<li>1 如果 checkpoint 目录 @checkpoint_dir 存在，则退出；</li>
<li>2 创建 临时目录 “@checkpoint_dir + .tmp”；</li>
<li>3 根据 live file 的名称获取文件的类型，根据类型不同分别进行复制；

<ul>
<li>3.1 如果 type 是 SST 则进行 hard link，hark link 失败再尝试进行 Copy；</li>
<li>3.2 如果 type 是其他类型则直接进行 Copy，如果 type 是 kDescriptorFile（manifest 文件）还需要指定文件的大小；</li>
</ul></li>
<li>4 单独创建一个 CURRENT 文件，其内容是 manifest 文件的名称；</li>
<li>5 备份 WAL 文件；

<ul>
<li>5.1 如果文件的类型是归档 WAL，则拒绝备份；</li>
<li>5.2 通过 LogFile::StartSequence() 获取 WAL 初始 SequenceNumber，如果这个 SequenceNumber 小于备份开始时的系统 SequenceNumber，则拒绝备份；</li>
<li>5.3 如果 WAL 文件是最后文件集合的最后一个，则 Copy 文件，且只复制文件在备份开始时的文件 size，以防止复制过多的操作指令；</li>
<li>5.3 如果备份文件和原始文件在同一个文件系统上，则进行 hard link，否则进行 Copy；</li>
</ul></li>
<li>6 允许文件删除；</li>
<li>7 把临时目录 “@checkpoint<em>dir + .tmp” 重命名为 @checkpoint</em>dir，并执行 fsync 操作，把数据刷到磁盘。</li>
</ul>

<p>注：BackupCentent 中别的文件如 CURRENT、SST、Manifest 都是文件名称，唯独 WAL 文件传递了相关的句柄 <a href="https://github.com/facebook/rocksdb/blob/master/include/rocksdb/transaction_log.h#L32">LogFile</a>。</p>

<h4 id="toc_10">2.2 BackupEngine</h4>

<hr>

<p>基于 DBNemoCheckpoint，nemo:BackupEngine 提供了一个异步备份五种类型数据文件的接口，其定义如下：</p>

<div><pre><code class="language-cpp">    // Arguments which will used by BackupSave Thread
    // p_engine for BackupEngine handler
    // backup_dir
    // key_type kv, hash, list, set or zset
    struct BackupSaveArgs {
        void *p_engine;
        const std::string backup_dir;
        const std::string key_type;
        Status res;
    };

    struct BackupContent {
        std::vector&lt;std::string&gt; live_files;
        rocksdb::VectorLogPtr live_wal_files;
        uint64_t manifest_file_size = 0;
        uint64_t sequence_number = 0;
    };

    class BackupEngine {
        public:
            ~BackupEngine();
            // 调用 BackupEngine::NewCheckpoint 为五种数据类型分别创建响应的 DBNemoCheckpoint 放入 engines_，
            // 同时创建 BackupEngine 对象
            static Status Open(nemo::Nemo *db, BackupEngine** backup_engine_ptr);
            // 调用 DBNemoCheckpointImpl::GetCheckpointFiles 获取五种类型需要备份的 快照内容 存入 backup_content_
            Status SetBackupContent();
            // 创建五个线程，分别调用 CreateNewBackupSpecify 进行数据备份
            Status CreateNewBackup(const std::string &amp;dir);

            void StopBackup();
            // 调用 DBNemoCheckpointImpl::CreateCheckpointWithFiles 执行具体的备份任务
            // 这个函数之所以类型是 public 的，是为了在 线程函数ThreadFuncSaveSpecify 中能够调用之
            Status CreateNewBackupSpecify(const std::string &amp;dir, const std::string &amp;type);
        private:
            BackupEngine() {}

            std::map&lt;std::string, rocksdb::DBNemoCheckpoint*&gt; engines_; // 保存每个类型的 checkpoint 对象
            std::map&lt;std::string, BackupContent&gt; backup_content_; // 保存每个类型需要复制的 快照内容
            std::map&lt;std::string, pthread_t&gt; backup_pthread_ts_; // 保存每个类型执行备份任务的线程对象

            // 调用 rocksdb::DBNemoCheckpoint::Create 创建 checkpoint 对象
            Status NewCheckpoint(rocksdb::DBNemo *tdb, const std::string &amp;type);
            // 获取每个类型的数据目录
            std::string GetSaveDirByType(const std::string _dir, const std::string&amp; _type) const {
                std::string backup_dir = _dir.empty() ? DEFAULT_BK_PATH : _dir;
                return backup_dir + ((backup_dir.back() != &#39;/&#39;) ? &quot;/&quot; : &quot;&quot;) + _type;
            }
            Status WaitBackupPthread();
    };</code></pre></div>

<p><code>nemo::BackupEngine</code> 对外的主要接口是 Open、SetBackupContent、CreateNewBackup 和 StopBackup，分别用于 创建 BackupEngine 对象、获取快照内容、执行备份任务和停止备份任务。</p>

<h4 id="toc_11">2.3 Bgsave</h4>

<hr>

<p><code>PikaServer::Bgsave</code> 是 redis 命令 bgsave 的响应函数，通过调用 <code>nemo::BackupEngine</code> 相关接口执行备份任务，下面先分别介绍其先关的函数接口。</p>

<h4 id="toc_12">2.3.1 PikaServer::InitBgsaveEnv</h4>

<hr>

<p>这个函数用于创建数据备份目录，其流程为：</p>

<ul>
<li>1 获取当前时间，以 <code>%Y%m%d%H%M%S</code> 格式序列化为字符串；</li>
<li>2 创建目录 <code>pika.conf:dump-path/%Y%m%d</code>，如果目录已经存在，则删除之；</li>
<li>3 删除目录 <code>pika.conf:dump-path/_FAILED</code>。</li>
</ul>

<p>注意上面第二步的备份目录，之所以最终目录只有年月日信息，是因为最终只用了前 8 个字符串作为目录名称。</p>

<h4 id="toc_13">2.3.2 PikaServer::InitBgsaveEngine</h4>

<p>这个函数用于创建 BackupEngine 对象并进行获取五种数据类型的快照内容，其流程为：</p>

<ul>
<li>1 调用 <code>nemo::BackupEngine::Open</code> 创建 nemo::BackupEngine 对象；</li>
<li>2 通过 <code>PikaServer::rwlock_::WLock</code> 进行数据写入 RocksDB::DB 阻止；</li>
<li>3 获取当前 Binlog 的 filenum 和 offset；</li>
<li>4 调用 <code>nemo::BackupEngine:: SetBackupContent</code> 获取快照内容；</li>
<li>5 通过 <code>PikaServer::rwlock_::UnLock</code> 取消数据写入 RocksDB::DB 阻止。</li>
</ul>

<p><code>PikaClientConn::DoCmd</code> 在执行写命令的时候，会先调用 <code>g_pika_server-&gt;RWLockReader()</code> 尝试加上读锁，如果正在执行 Bgsave 则此处就会阻塞等待。 </p>

<h4 id="toc_14">2.3.3 PikaServer::RunBgsaveEngine</h4>

<p>这个函数用于执行具体的备份任务，其流程为：</p>

<ul>
<li>1 调用 <code>PikaServer::InitBgsaveEnv</code> 初始化 BGSave 需要的目录环境；</li>
<li>2 调用 <code>PikaServer:: InitBgsaveEngine</code> 创建 nemo::BackupEngine 对象和获取快照内容；</li>
<li>3 调用 <code>nemo::BackupEngine::CreateNewBackup</code> 执行备份任务。</li>
</ul>

<h4 id="toc_15">2.3.4 PikaServer::DoBgsave</h4>

<p>这个函数是 Bgsave 线程的执行体，其流程为：</p>

<ul>
<li>1 调用 <code>PikaServer::RunBgsaveEngine</code> 执行数据备份；</li>
<li>2 把执行备份任务时长、本机 hostinfo、binlog filenum 和 binlog offset 写入 <code>pika.conf:dump-path/%Y%m%d/info</code> 文件；</li>
<li>3 如果备份失败，则把 <code>pika.conf:dump-path/%Y%m%d</code> 重命名为 <code>pika.conf:dump-path/%Y%m%d_FAILED</code>；</li>
<li>4 把 <code>bgsave_info_.bgsaving</code> 置为 false。</li>
</ul>

<h4 id="toc_16">2.3.4 PikaServer::Bgsave</h4>

<p>作为命令 bgsave 的响应函数，其流程非常简单：</p>

<ul>
<li>1 如果 <code>bgsave_info_.bgsaving</code> 值为 true，则退出，否则把其值置为 true；</li>
<li>2 启动 <code>PikaServer::bgsave_thread_</code>，通过调用 <code>PikaServer::DoBgsave</code> 函数完成备份任务。</li>
</ul>

<h3 id="toc_17">3 Blackwidow</h3>

<hr>

<p>Pika 存储引擎的最基本作用就是把 Redis 的数据结构映射为 RocksDB 的 KV 数据存入其中。本节主要分析 Pika 最新版的存储引擎 Blackwidow，作为对比需要稍微提及其前一个版本 Nemo。</p>

<h4 id="toc_18">3.1 Nemo</h4>

<hr>

<p>Nemo 自身并不直接使用 RocksDB，而是使用 nemo-rocksdb - - - 一个对 RocksDB 进行了一层薄薄封装的存储层。</p>

<p>nemo-rocksdb 的主要类 DBNemo 继承自 rocksdb::StackableDB，用于替代 rocksdb::DB，主要作用是给 KV 的 Key 添加 timestamp 和 version 以及 Key 的类型信息，以实现 Redis 对数据的时限【称之为 ttl】要求：在 RocksDB 进行 compaction 的时候预先检查数据是否过期，过期则直接淘汰。</p>

<p>RocksDB 进行 compaction 的时候需要对每个 key 调用留给使用者的接口 CompactionFilter 以进行过滤：让用户解释当前 key 是否还有效。nemo-rocksdb 封装了一个 NemoCompactionFilter 以实现过时数据的检验，其主要接口是 rocksdb:CompactionFilter::Filter。RocksDB 在进行 compaction 还会调用另一个预备给用户的接口 rocksdb::MergeOperator，以方便用户自定义如何对同一个 key 的相关操作进行合并。</p>

<p>nemo-rocksdb 一并重新封装了一个可以实现 <strong>更新</strong> 意义的继承自 rocksdb::MergeOperator 的 NemoMergeOperator，以在 RocksDB 进行 Get 或者 compaction 的时候对 key 的一些写或者更行操作合并后再进行，以提高效率。至于 rocksdb::MergeOperator 的使用，见<a href="http://alexstocks.github.io/html/rocksdb.html">参考文档6</a>。</p>

<h4 id="toc_19">3.2 Blackwidow Filter</h4>

<hr>

<p>相对于需要对 RocksDB 封装了一层的 nemo-rocksdb 的存储引擎 Nemo，Blackwidow 则更多地使用了 RocksDB 暴露出来的一些常用接口实现了 Redis 数据到 RocksDB KV 的映射。</p>

<p>Blackwidow 的数据组织格式与 Nemo 做了两个大的调整：</p>

<ul>
<li>四种特殊数据类型的 meta 与 data 分离分别存入两个 ColumnFamily；

<ul>
<li>meta 存入 default ColumnFamily;</li>
<li>hashtable 和 list 与 zset 的值存入 ”data_cf”;</li>
<li>set 的值存入 &quot;member_cf”;</li>
<li>zset 的 score 存入 &quot;score_cf”；</li>
</ul></li>
<li>五种数据类型与 RocksDB 的 KV 映射形式进行了重新调整，详见<a href="https://github.com/qihoo360/pika/wiki/pika-blackwidow%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F">参考文档8</a>;</li>
</ul>

<p>rocksdb::CompactionFilter 调用暴露给用户的接口 CompactionFilter::Filter 的时候，需要用户自己对相关数据的含义进行解释并处理，下面分小节介绍相关数据类操作。</p>

<h5 id="toc_20">3.2.1 blackwidow::InternalValue</h5>

<hr>

<p>base<em>value</em>format.h:blackwidow::InternalValue 用于存储 string 类型的 Key 和 其他四种类型的 meta Key，其主要类成员如下：</p>

<div><pre><code class="language-cpp">class InternalValue {
 public:
  virtual size_t AppendTimestampAndVersion() = 0;
 protected:
  char space_[200];
  char* start_;
  Slice user_value_;  // 用户原始 key
  int32_t version_;
  int32_t timestamp_;
};</code></pre></div>

<p>blackwidow::InternalValue 主要的接口是 Encode，其作用是把 key 的相关信息序列化成一个字节流，其工作流程如下：</p>

<ul>
<li>1 若 <code>key + timestamp + version</code> 拼接后的总长度不大于 200B，则 InternalValue::start_ = InternalValue::space<em>，即使用 InternalValue::space</em> 存储序列化后的字节流，否则就在堆上分配一段内存用于存储字节流；</li>
<li>2 调用虚接口 blackwidow:AppendTimestampAndVersion 对 <code>key + timestamp + version</code> 进行序列化并存入 InternalValue::start_。</li>
</ul>

<p>继承自 blackwidow::InternalValue 的 base<em>meta</em>value_format.h:BaseMetaValue 主要用于对 meta key进行序列化。 </p>

<h5 id="toc_21">3.2.2 blackwidow::ParsedInternalValue 与 blackwidow::BaseMetaFilter</h5>

<hr>

<p>base<em>value</em>format.h:blackwidow::ParsedInternalValue 用于对 string 类型的 Value 和 其他四种类型的 meta Value 进行反序列化，其主要类成员如下：</p>

<div><pre><code class="language-cpp">class ParsedInternalValue {
 public:
  // 这个构造函数在 rocksdb::DB::Get() 之后会被调用，
  // 用户可能在此处对读取到的值修改 timestamp 和 version，
  // 所以需要把 value 的指针赋值给 value_
  explicit ParsedInternalValue(std::string* value) :
    value_(value),
    version_(0),
    timestamp_(0) {
  }

  // 这个函数在 rocksdb::CompactionFilter::Filter() 之中会被调用，
  // 用户仅仅仅对 @value 进行分析即可，不会有写动作，所以不需要
  // 把 value 的指针赋值给 value_
  explicit ParsedInternalValue(const Slice&amp; value) :
    value_(nullptr),
    version_(0),
    timestamp_(0) {
  }
 protected:
  virtual void SetVersionToValue() = 0;
  virtual void SetTimestampToValue() = 0;
  std::string* value_;
  Slice user_value_;  // 用户原始 value
  int32_t version_;
  int32_t timestamp_;
};</code></pre></div>

<p>继承自 blackwidow::ParsedInternalValue 的 <strong>base_meta_value_format.h:blackwidow::ParsedBaseMetaValue</strong> 主要用于对 meta value 进行反序列化，需要注意的是 blackwidow::ParsedBaseMetaValue 多了一个 blackwidow::ParsedBaseMetaValue::count_ 成员，用于记录集合中成员【field/field】的数目，这个数值一般位于字节流的前四个字节。</p>

<p>继承自 rocksdb::CompactionFilter 的 <strong>base_filter.h:blackwidow::BaseMetaFilter</strong> 在调用其 Filter 接口的时候，就使用 blackwidow::ParsedInternalValue 对 meta value 进行了解析处理，其工作流程如下：</p>

<ul>
<li>1 获取当前时间；</li>
<li>2 使用 blackwidow::ParsedBaseMetaValue 对 meta value 进行解析；</li>
<li>3 若 <strong><em>meta value timestamp 不为零</em></strong> 且 <strong><em>meta value timestamp 小于当前时间</em></strong> 且 <strong><em>meta value version 小于当前时间</em></strong>，则数据可以淘汰；</li>
<li>4 若 <strong><em>meta value count 为零</em></strong> 且 <strong><em>meta value version 小于当前时间</em></strong>，则数据可以淘汰；</li>
<li>5 否则数据仍然有效，不能淘汰。</li>
</ul>

<p>使用 <strong>blackwidow::BaseMetaFilter</strong> 的 <strong>blackwidow::BaseMetaFilterFactory</strong> 会被设置为 hashtable/set/zset 三种数据结构 meta ColumnFamily 的 ColumnFamilyOptions 的 compaction<em>filter</em>factory。</p>

<h5 id="toc_22">3.2.3 blackwidow::BaseDataKey</h5>

<hr>

<p>base<em>data</em>key_format.h:blackwidow::BaseDataKey 用于存储 hashtable/zset/set 三种类型 Data ColumnFamily 的 Key【下文称为 data key】，其主要类成员如下：</p>

<div><pre><code class="language-cpp">class BaseDataKey {
 public:
  const Slice Encode();
 private:
  char space_[200];
  char* start_;
  Slice key_;  // hashtable/zset/set key
  int32_t version_;
  Slice data_;  // field/member
};</code></pre></div>

<p>blackwidow::BaseDataKey 主要的接口是 Encode，其作用是把 KV Key 的相关信息序列化成一个字节流，其工作流程如下：</p>

<ul>
<li>1 若 <code>key size(4B) + key + version + field</code> 拼接后的总长度不大于 200B，则 BaseDataKey::start_ = BaseDataKey::space<em>，即使用 InternalValue::space</em> 存储序列化后的字节流，否则就在堆上分配一段内存用于存储字节流；</li>
<li>2 把 key size 存入字节流前 4 字节；</li>
<li>3 存入 key；</li>
<li>4 存入 version；</li>
<li>5 存入 field。</li>
</ul>

<h5 id="toc_23">3.2.4 blackwidow::ParsedBaseDataKey 与 blackwidow::BaseDataFilter</h5>

<hr>

<p>base<em>data</em>key_format.h:blackwidow::ParsedBaseDataKey 用于对 hashtable/zset/set 三种类型的 data key 进行反序列化，其主要类成员如下：</p>

<div><pre><code class="language-cpp">class ParsedBaseDataKey {
 protected:
  Slice key_;
  int32_t version_;
  Slice data_;
};</code></pre></div>

<p>其主要反序列化解析动作在构造函数中完成，此处就不再详细分析其工作流程。</p>

<p>继承自 rocksdb::CompactionFilter 的 <strong>base_filter.h:blackwidow::BaseDataFilter</strong> 主要用于对 data KV 进行解析，其主要成员如下：</p>

<div><pre><code class="language-cpp">class BaseDataFilter {
 private:
  rocksdb::DB* db_;  // 所在的 DB
  std::vector&lt;rocksdb::ColumnFamilyHandle*&gt;* cf_handles_ptr_; // 所在的 ColumnFamily
  rocksdb::ReadOptions default_read_options_;
  mutable std::string cur_key_;
  mutable bool meta_not_found_;
  mutable int32_t cur_meta_version_;
  mutable int32_t cur_meta_timestamp_;
};</code></pre></div>

<p>在调用其 Filter 接口的时候，就使用 blackwidow::ParsedBaseDataKey 对 data key 进行了解析处理，其工作流程如下：</p>

<ul>
<li>1 使用 blackwidow::ParsedBaseDataKey 对 data key 进行解析；</li>
<li><p>2 若 cur<em>key</em> 与 hashtable/zset/set key 不相等，则从 meta ColumnFamily 中获取 hashtable/zset/set 对应的 meta value；</p>

<ul>
<li>2.1 使用 ParsedBaseMetaValue 解析 meta value；</li>
<li>2.2 获取 hashtable/zset/set 当前的 cur<em>meta</em>version_ 与 cur<em>meta</em>timestamp_；</li>
<li>2.3 获取不到 meta value 则意味着当前 data KV 可以淘汰；</li>
</ul></li>
<li><p>3 获取系统当前时间；</p></li>
<li><p>4 若 <strong><em>cur<em>meta</em>timestamp_ 不为零 且 cur<em>meta</em>timestamp_ 小于 系统当前时间</em></strong>，则数据可以淘汰；</p></li>
<li><p>5 若 <strong><em>data key 的 version 小于 cur<em>meta</em>version_</em></strong>，则数据可以淘汰；</p></li>
<li><p>6 否则数据仍然有效，不能淘汰。</p></li>
</ul>

<p>使用 <strong>blackwidow::BaseDataFilter</strong> 的 <strong>blackwidow::BaseDataFilterFactory</strong> 会被设置为 hashtable/set/zset 三种数据结构 data ColumnFamily 的 ColumnFamilyOptions 的 compaction<em>filter</em>factory。</p>

<h4 id="toc_24">3.3 Blackwidow Strings</h4>

<hr>

<p>不同于其他四种数据结构，Strings 因其数据结构比较简单，不需要 meta 数据，所以的数据直接存入默认的 ColumnFamily，相关的 Blackwidow 类在此节单独列明。</p>

<h5 id="toc_25">3.3.1 blackwidow::StringsValue</h5>

<hr>

<p><strong>strings<em>value</em>format.h:blackwidow::StringsValue</strong> 继承自 <strong>blackwidow::InternalValue</strong>，其作用自然是序列化 KV value，其主要接口 AppendTimestampAndVersion 代码如下： </p>

<div><pre><code class="language-cpp">class StringsValue : public InternalValue {
 public:
  explicit StringsValue(const Slice&amp; user_value) :
    InternalValue(user_value) {
  }
  virtual size_t AppendTimestampAndVersion() override {
    size_t usize = user_value_.size();
    char* dst = start_;
    memcpy(dst, user_value_.data(), usize);
    dst += usize;
    EncodeFixed32(dst, timestamp_);
    return usize + sizeof(int32_t);
  }
};</code></pre></div>

<p>从上面代码可以看出，Strings 没有 version 概念。</p>

<h5 id="toc_26">3.3.2 blackwidow::ParsedStringsValue 与 blackwidow::StringsFilter</h5>

<hr>

<p><strong>strings<em>value</em>format.h:blackwidow::ParsedStringsValue</strong> 继承自 <strong>blackwidow::ParsedInternalValue</strong>，其作用自然是反序列化 KV value，获取 V 与 timestamp。</p>

<p>继承自 rocksdb::CompactionFilter 的 <strong>strings_filter.h:blackwidow::StringsFilter</strong> 通过 <strong>blackwidow::ParsedStringsValue</strong> 对 Strings KV 进行解析，其 Filter 接口依据 V 中的 timestamp 与系统当前时间进行比较，如果 V 的 timestamp 小于系统当前时间，则数据过时可以淘汰。</p>

<p>使用 <strong>blackwidow:: StringsFilter</strong> 的 <strong>blackwidow::StringsFilterFactory</strong> 会被设置为 Strings 的 default ColumnFamily 的 ColumnFamilyOptions 的 compaction<em>filter</em>factory。</p>

<h2 id="toc_27">参考文档</h2>

<ul>
<li>1 <a href="https://github.com/Qihoo360/pika/wiki/%E4%BD%BF%E7%94%A8binlog%E8%BF%81%E7%A7%BB%E6%95%B0%E6%8D%AE%E5%B7%A5%E5%85%B7">使用binlog迁移数据工具</a></li>
<li>2 <a href="https://www.jianshu.com/p/01bd76eb7a93">pika主从复制原理之工作流程</a></li>
<li>3 <a href="https://www.jianshu.com/p/d969b6f6ae42">pika主从复制原理之binlog</a></li>
<li>4 <a href="https://github.com/qihoo360/pika/wiki/pika-%E5%BF%AB%E7%85%A7%E5%BC%8F%E5%A4%87%E4%BB%BD%E6%96%B9%E6%A1%88">Pika 快照式备份方案</a></li>
<li>5 <a href="http://yinqiwen.github.io/">杂感(2016-06)</a></li>
<li>6 <a href="http://alexstocks.github.io/html/rocksdb.html">RocksDB 笔记</a></li>
<li>7 <a href="http://kernelmaker.github.io/pika-muli-idc">pika 跨机房同步设计</a></li>
<li>8 <a href="https://github.com/qihoo360/pika/wiki/pika-blackwidow%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F">pika blackwidow引擎数据存储格式</a></li>
</ul>

<h2 id="toc_28">扒粪者-于雨氏</h2>

<blockquote>
<p>2018/09/07，于雨氏，初作此文于西二旗。</p>

<p>2018/09/15，于雨氏，于西二旗添加第二节 “数据备份”。</p>
</blockquote>


<!-- baidu statistic start -->
<script>
var _hmt = _hmt || [];
(function() {
	  var hm = document.createElement("script");
	    hm.src = "https://hm.baidu.com/hm.js?170a8df8802fbc47c7acc272d270979c";
		  var s = document.getElementsByTagName("script")[0];
		    s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- baidu statistic end -->

<!-- Gitalk start -->
<link rel="stylesheet" href="https://unpkg.com/gitalk@latest/dist/gitalk.css">
<script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script>
<div id="gitalk-container"></div>
<script type="text/javascript">
var gitalk = new Gitalk({
	clientID: '6211d8b94a8106bed6b0',
	clientSecret: 'bf77ca26c237eabbd45169e01bf03a5e96a1b26f',
	repo: 'alexstocks.github.io',
	owner: 'AlexStocks',
	admin: ['AlexStocks'],
	id: window.location.pathname,
	distractionFreeMode: true
});
gitalk.render('gitalk-container');
</script>
<!-- Gitalk end -->




</body>

</html>
